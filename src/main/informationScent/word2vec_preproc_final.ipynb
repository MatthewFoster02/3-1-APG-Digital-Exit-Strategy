{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF class. Contains methods which can easily access relevant information from the csv file which the class reads, which represents the TF-IDF model.\n",
    "\n",
    "TF-IDF stands for Term Frequency - Inverse Document Frequency. It gives each keyword-document combination a weight, based on how often it occurs in the document, but also based on how often it occurs in all documents. This ensures that words which occur often in many documents do not receive high weights, rather only words which are relatively unique to a document and occur frequently in that document get high weights.\n",
    "\n",
    "If all documents are about topic A, we do not care that document x contains is about document A.\n",
    "\n",
    "Implementation wise, the TF-IDF is a csv file with each row representing a document and each column representing a keyword, the value at i,j represents the weight that keyword j has in document i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfIdf:\n",
    "    def __init__(self):\n",
    "        # Loading in the tf-idf\n",
    "        # Rows are documents, the first column is the document id\n",
    "        # Columns are keywords, the first row is the keyword id\n",
    "        self.tfidf = pd.read_csv(\"C:/Users/mattf/Downloads/tf_idf_new.csv\") #your_filepath_here\n",
    "\n",
    "        # Loading in the keywords\n",
    "        # Two columns, column 1 is id, column 2 is keyword\n",
    "        # We can access the weight in the tf-idf by first accessing the id number from the keyword file\n",
    "        self.tfidf_keywords = pd.read_csv(\"C:/Users/mattf/Downloads/tf_idf_keywords_new.csv\") #your_filepath_here\n",
    "        self.tfidf_keywords.columns = [\"id\", \"keyword\"]\n",
    "\n",
    "    def get_id_by_keyword(self, keyword):\n",
    "        return self.tfidf_keywords.id.iloc[self.tfidf_keywords[self.tfidf_keywords.keyword == keyword].index].values[0]\n",
    "\n",
    "\n",
    "    def get_keyword_by_id(self, id):\n",
    "        return self.tfidf_keywords.loc[self.tfidf_keywords.id==id, 'keyword'].iloc[0]\n",
    "    # Method to get the tfidf value of a keyword in a page\n",
    "    def get_tf_idf_value(self, page_id, keyword):\n",
    "        keyword_id = self.get_id_by_keyword(keyword)\n",
    "        return self.tfidf.iloc[page_id-1, keyword_id + 1]\n",
    "\n",
    "    def get_all_keywords_by_id(self, page_id):\n",
    "        keywords = self.tfidf.iloc[page_id-1][1:].values\n",
    "        return keywords\n",
    "\n",
    "    def get_all_keywords_by_id_normalized(self, page_id):\n",
    "        keywords_weights = self.get_all_keywords_by_id(page_id)\n",
    "        sum_value = sum(keywords_weights)\n",
    "        if sum_value == 0:\n",
    "            return keywords_weights\n",
    "        keywords_weights_normalized = keywords_weights/sum_value\n",
    "        return keywords_weights_normalized\n",
    "\n",
    "    def get_number_of_keywords(self):\n",
    "        return self.tfidf_keywords.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfIdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the dataset of visits, used to test the information need prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "generic = lambda x: literal_eval(x)\n",
    "\n",
    "conv = {'url_id_path': generic,\n",
    "        'seconds_spent_path': generic}\n",
    "df = pd.read_csv('C:/Users/mattf/Downloads/visits_matthew.csv', converters=conv) #your_filepath_here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create seperate dataframes for the paths taken and for the seconds spent on each url.\n",
    "In paths, index 0 gives a list of urls, the equivalent index 0 in seconds represents how many seconds the user spent on the equal index urls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360511\n"
     ]
    }
   ],
   "source": [
    "visit_ids = df['visitor_id[visit_number]']\n",
    "paths = df.url_id_path\n",
    "#secs_spent = df.seconds_spent_path NOT USING TIME SO I GUESS THIS IS UNNECESSARY\n",
    "print(len(paths)) # SHOULD BE 360511 FOR DINO, LAURENCE, AND I - BUT 360513 FOR MATTHIJS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT SURE WHAT THIS IS FOR\n",
    "#list = tfidf.get_all_keywords_by_id(1)\n",
    "#for i in range(0, len(list)):\n",
    "#    if list[i]!=0:\n",
    "#        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ")Simplified implementation of IUNIS, which simply looks at the keywords which the user comes across, does not use the adjacency matrix, proximal cues, and spreading activation.\n",
    "It seems that these are mostly important for WUFIS, which we are not interested in, thus implementing it is not worth the effort.\n",
    "\n",
    "Input is the ordered list of page id's that the user has visited.\n",
    "For each page, take the TF-IDF values from that page, which is a row of keywords, with either 0 if the keyword is not present or [0,1] if it is.\n",
    "\n",
    "Then, for each next page, we add the TF-IDF values onto the existing weights.\n",
    "\n",
    "Eventually we can sort the list and we will have the keywords with the highest weight on top, which then represents the most relevant keywords in the visit up until now, and acts as our prediction for what the user is looking for.\n",
    "Instead of only looking at the user alone, we can also compare their information need to information needs of other users, and by looking at where the other users end up, we can predict where our user might want to end up.\n",
    "\n",
    "Optionally, a decay factor can be introdced. Essentially, every iteration, all values are divided by a factor, meaning that keywords on recent pages are more biased and weighted more heavily.\n",
    "\n",
    "It is also possible to give more weight to pages based on how much time the user spent on that page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "#Method which essentially just takes the keywords and their weights on each page the user visits, continues to the next url, and then sums the weights.\n",
    "#Be sure to realise that only passing a path will execute the normal implementation, which simply sums all new tf-idf values of each url.\n",
    "#This is because the default decay_factor value is 1, meaning that nothing changes, and the default secs is None, if that is None, the if statement with secs is never entered and we do not take time into account.\n",
    "#Sorted gives an option to sort the vector before returning, this puts the most relevant keywords on top, but does not allow for easy similarity comparison, thus should only be used if this is not the intention\n",
    "def find_keyword_weights(path, decay_factor=1, secs=None, sorted=False):\n",
    "    #initiate array of weights (weights start as 0, size=num of keywords)\n",
    "    weights = np.zeros(tfidf.get_number_of_keywords())\n",
    "    #ensures all output is visible\n",
    "    pd.options.display.max_rows = 0\n",
    "    #Iterate over each page url\n",
    "    for i in range(len(path)):\n",
    "        #Divide the weights by the decay factor (1 by default, no decay in that case)\n",
    "        weights = weights/decay_factor\n",
    "        #Initialize the tf-idf weights of the current page\n",
    "        new_weights = tfidf.get_all_keywords_by_id_normalized(path[i])\n",
    "        #If we are taking time into account, multiply the weights by the amount of time spent on this page\n",
    "        if secs is not None:\n",
    "            new_weights*=secs[i]\n",
    "        #Add the new weights to the existing weights\n",
    "        weights += new_weights\n",
    "        #Normalize such that the largest value becomes 1\n",
    "        if max(weights)==0:\n",
    "            continue\n",
    "        weights /= max(weights)\n",
    "    #Option to sort s.t. max weights are on top.\n",
    "    if sorted:\n",
    "        df = DataFrame(weights, columns=['weights'])\n",
    "        return df.sort_values(by=['weights'], ascending=False)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method which calculates keywords of a visit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculatePercentComplete(decimal_complete):\n",
    "    return round(decimal_complete, 3)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_keywords_of_complete_visit(visit_ids, paths):\n",
    "    total_paths = len(paths)\n",
    "    all_visits = {}\n",
    "    keys = []\n",
    "    keywords_weight_dict = {}\n",
    "    keywords_weights_top_10 = {}\n",
    "    for i in range(tfidf.get_number_of_keywords()):\n",
    "        keys.append(tfidf.get_keyword_by_id(i))\n",
    "    \n",
    "    previous_percent = -1\n",
    "    for index, path in enumerate(paths):\n",
    "        weights = find_keyword_weights(path, decay_factor=28) #!!!! CAREFUL make sure you have the correct parameters\n",
    "        keywords_weights_dict = dict(zip(keys, weights))\n",
    "        series = pd.Series(keywords_weights_dict)\n",
    "        series.sort_values(axis=0, ascending=False, inplace=True, kind='quicksort', na_position='last', ignore_index=False, key=None)\n",
    "        keywords_weights_dict_sorted = series.to_dict()\n",
    "        keywords_weights_top_10 = dict(zip(list(keywords_weights_dict_sorted.keys())[:10], list(keywords_weights_dict_sorted.values())[:10]))\n",
    "        all_visits[visit_ids[index]] = keywords_weights_top_10\n",
    "        percent = calculatePercentComplete(index/total_paths)\n",
    "        if percent != previous_percent:\n",
    "            print(f\"{percent}% done...\")\n",
    "            previous_percent = percent\n",
    "\n",
    "    print(\"DONE!\")\n",
    "    return all_visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% done...\n",
      "0.1% done...\n",
      "0.2% done...\n",
      "0.3% done...\n",
      "0.4% done...\n",
      "0.5% done...\n",
      "0.6% done...\n",
      "0.7000000000000001% done...\n",
      "0.8% done...\n",
      "0.8999999999999999% done...\n",
      "1.0% done...\n",
      "1.0999999999999999% done...\n",
      "1.2% done...\n",
      "1.3% done...\n",
      "1.4000000000000001% done...\n",
      "1.5% done...\n",
      "1.6% done...\n",
      "1.7000000000000002% done...\n",
      "1.7999999999999998% done...\n",
      "1.9% done...\n",
      "2.0% done...\n",
      "2.1% done...\n"
     ]
    }
   ],
   "source": [
    "all_keywords_and_weights_with_visit_ids = compute_keywords_of_complete_visit(visit_ids, paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:/Users/mattf/Downloads/matthew_keywords.json', 'w') as jf: #your_file_name_here!!!!!\n",
    "    json.dump(all_keywords_and_weights_with_visit_ids, jf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
