{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# functions\n",
    "def add_address_of_data(given_address): # could be useful for easily allowing others to use this file\n",
    "    return \"\".join([given_address, \"/visit_meaning_vectors/visit_meanings.csv\"])\n",
    "\n",
    "# calculating Euclidean distance\n",
    "def calculate_prob_of_visit_with_sorting(given_visit, given_pca):\n",
    "    # assuming given visit is a meaning vector, with number of variables same as number of PCs\n",
    "\n",
    "    given_pca_sorted = given_pca.sort_values(by=[\"Labels\"]) # sorting given PCA list by labels\n",
    "    prev_label = 0\n",
    "    current_label = 0\n",
    "    lowest_dist_to_visit = 0\n",
    "    current_labels_dist = 0\n",
    "    total_dist = 0\n",
    "    lowest_dist_to_visits_label = 0\n",
    "\n",
    "    for x in range(0, len(given_pca_sorted)):\n",
    "        current_label = given_pca_sorted.at[given_pca_sorted.index[x], \"Labels\"] # get current label\n",
    "\n",
    "        # extracting required variables of the point in PCA data\n",
    "        temp_point = given_pca_sorted.iloc[x]\n",
    "        temp_point = temp_point.tolist()\n",
    "        temp_point = temp_point[:-1]\n",
    "\n",
    "        if current_label >= 0: # to skip \"-1\" labels\n",
    "            if current_label == prev_label: current_labels_dist += np.linalg.norm(np.array(given_visit) - np.array(temp_point)) # performing the euclidean distance calculation\n",
    "            else: # have arrived to next label in the sorted PCA, so check final values\n",
    "                if current_labels_dist < lowest_dist_to_visit:\n",
    "                    lowest_dist_to_visit = current_labels_dist\n",
    "                    lowest_dist_to_visits_label = current_label\n",
    "                prev_label = current_label\n",
    "                total_dist += current_labels_dist\n",
    "                current_labels_dist = 0 # since we are going to next label, reset distance\n",
    "\n",
    "    if total_dist == 0: print(len(given_visit))\n",
    "    return 1 - (lowest_dist_to_visits_label / total_dist), lowest_dist_to_visits_label\n",
    "\n",
    "# calculating Euclidean distance and assumes that \"given_pca\" is already sorted\n",
    "def calculate_prob_of_visit_without_sorting(given_visit, given_pca_sorted):\n",
    "    # assuming given visit is a meaning vector, with number of variables same as number of PCs\n",
    "\n",
    "    prev_label = 0\n",
    "    current_label = 0\n",
    "    lowest_dist_to_visit = 0\n",
    "    current_labels_dist = 0\n",
    "    total_dist = 0\n",
    "    lowest_dist_to_visits_label = 0\n",
    "\n",
    "    for x in range(0, len(given_pca_sorted)):\n",
    "        current_label = given_pca_sorted.at[given_pca_sorted.index[x], \"Labels\"] # get current label\n",
    "\n",
    "        # extracting required variables of the point in PCA data\n",
    "        temp_point = given_pca_sorted.iloc[x]\n",
    "        temp_point = temp_point.tolist()\n",
    "        temp_point = temp_point[:-1]\n",
    "\n",
    "        if current_label >= 0: # to skip \"-1\" labels\n",
    "            if current_label == prev_label: current_labels_dist += np.linalg.norm(np.array(given_visit) - np.array(temp_point)) # performing the euclidean distance calculation\n",
    "            else: # have arrived to next label in the sorted PCA, so check final values\n",
    "                if current_labels_dist < lowest_dist_to_visit:\n",
    "                    lowest_dist_to_visit = current_labels_dist\n",
    "                    lowest_dist_to_visits_label = current_label\n",
    "                prev_label = current_label\n",
    "                total_dist += current_labels_dist\n",
    "                current_labels_dist = 0 # since we are going to next label, reset distance\n",
    "\n",
    "    if total_dist == 0: print(len(given_visit))\n",
    "    return 1 - (lowest_dist_to_visits_label / total_dist), lowest_dist_to_visits_label\n",
    "\n",
    "def get_avg_pca(given_pca):\n",
    "    given_pca_sorted = given_pca.sort_values(by=[\"Labels\"]) # sorting given PCA list by labels\n",
    "    list_of_avg_values = []\n",
    "    prev_label = 0\n",
    "    current_labels_dist = [0] * (len(given_pca.columns) - 1)\n",
    "    current_label_count = 0\n",
    "    for x in range(0, len(given_pca_sorted)):\n",
    "        current_label = given_pca_sorted.at[given_pca_sorted.index[x], \"Labels\"] # get current label\n",
    "\n",
    "\n",
    "        if current_label >= 0: # to skip \"-1\" labels\n",
    "            current_label_count += 1\n",
    "\n",
    "            if current_label == prev_label:\n",
    "                # extracting required variables of the point in PCA data\n",
    "                temp_point = given_pca_sorted.iloc[x]\n",
    "                temp_point = temp_point.tolist()\n",
    "                temp_point = temp_point[:-1]\n",
    "\n",
    "                current_labels_dist = [current_labels_dist[y] + temp_point[y] for y in range (0, len(current_labels_dist))]\n",
    "            else: # have arrived to next label in the sorted PCA, so check final values\n",
    "                prev_label = current_label\n",
    "                list_of_avg_values.append([current_labels_dist[z] / current_label_count for z in range (0, len(current_labels_dist))])\n",
    "                # reseting\n",
    "                current_labels_dist = [0] * (len(given_pca.columns)-1)\n",
    "                current_label_count = 0\n",
    "\n",
    "    temp_df = pd.DataFrame(list_of_avg_values, columns=[b for b in range(0, len(given_pca.columns)-1)])\n",
    "    return temp_df\n",
    "\n",
    "# calculating Euclidean distance and assumes that \"given_pca\" is already sorted and averaged\n",
    "def calculate_prob_of_visit_with_sorted_avg_pca(given_visit, given_avg_pca_sorted):\n",
    "    # assuming given visit is a meaning vector, with number of variables same as number of PCs\n",
    "\n",
    "    lowest_dist_to_visit = 0\n",
    "    current_labels_dist = 0\n",
    "    total_dist = 0\n",
    "    lowest_dist_to_visits_label = 0\n",
    "\n",
    "    for x in range(0, len(given_avg_pca_sorted)): # each index is a label\n",
    "        # extracting required variables of the point in PCA data\n",
    "        temp_point = given_avg_pca_sorted.iloc[x]\n",
    "        temp_point = temp_point.tolist()\n",
    "        temp_point = temp_point[:-1]\n",
    "\n",
    "        current_labels_dist += np.linalg.norm(np.array(given_visit) - np.array(temp_point)) # performing the euclidean distance calculation\n",
    "\n",
    "        if current_labels_dist < lowest_dist_to_visit:\n",
    "            lowest_dist_to_visit = current_labels_dist\n",
    "            lowest_dist_to_visits_label = x\n",
    "\n",
    "        total_dist += current_labels_dist\n",
    "        current_labels_dist = 0 # since we are going to next label, reset distance\n",
    "\n",
    "    if total_dist == 0: print(len(given_visit))\n",
    "    return 1 - (lowest_dist_to_visits_label / total_dist), lowest_dist_to_visits_label"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mParserError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-3-47a1d0f0761d>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;31m# main data\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mmy_address\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"C:/Users/dnaen/APG_data\"\u001B[0m  \u001B[1;31m# only this has to be modified\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 5\u001B[1;33m \u001B[0mdf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0madd_address_of_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmy_address\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[0;32m    608\u001B[0m     \u001B[0mkwds\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkwds_defaults\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    609\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 610\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0m_read\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    611\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    612\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    466\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    467\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0mparser\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 468\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mparser\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnrows\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    469\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    470\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36mread\u001B[1;34m(self, nrows)\u001B[0m\n\u001B[0;32m   1055\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mread\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnrows\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1056\u001B[0m         \u001B[0mnrows\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mvalidate_integer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"nrows\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnrows\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1057\u001B[1;33m         \u001B[0mindex\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcolumns\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcol_dict\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_engine\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnrows\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1058\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1059\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mindex\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36mread\u001B[1;34m(self, nrows)\u001B[0m\n\u001B[0;32m   2059\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mread\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnrows\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2060\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2061\u001B[1;33m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_reader\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnrows\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2062\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mStopIteration\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2063\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_first_chunk\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mpandas\\_libs\\parsers.pyx\u001B[0m in \u001B[0;36mpandas._libs.parsers.TextReader.read\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32mpandas\\_libs\\parsers.pyx\u001B[0m in \u001B[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32mpandas\\_libs\\parsers.pyx\u001B[0m in \u001B[0;36mpandas._libs.parsers.TextReader._read_rows\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32mpandas\\_libs\\parsers.pyx\u001B[0m in \u001B[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32mpandas\\_libs\\parsers.pyx\u001B[0m in \u001B[0;36mpandas._libs.parsers.raise_parser_error\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mParserError\u001B[0m: Error tokenizing data. C error: out of memory"
     ]
    }
   ],
   "source": [
    "# meaning vectors will be made of several hundreds of variables and each cluster will have a set of data points (each data point containing values of each of these variables (so a meaning vector)\n",
    "\n",
    "# main data\n",
    "my_address = \"C:/Users/dnaen/APG_data\"  # only this has to be modified\n",
    "df = pd.read_csv(add_address_of_data(my_address))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# will be used to get the labels\n",
    "combined_address = \"\".join([my_address, \"/cluster_paths.csv\"])\n",
    "df_with_labels = pd.read_csv(combined_address)\n",
    "\n",
    "df_with_labels.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# dividing the dataset into a feature set and corresponding labels\n",
    "features_X = df.pop(\"meaning_vectors\")\n",
    "labels_Y = df_with_labels.pop(\"cluster_label\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features_X = features_X.to_frame(name=\"meaning_vectors\") # converting from series to df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# expanding features df, because currently thes is only one column where each row contains a list of meaning values, to make it work for PCA method defined in sklearn library, it needs to be a df where each cell is only one number\n",
    "features_X_expanded = features_X.meaning_vectors.str.split(\",\",expand=True,)\n",
    "features_X_expanded.head(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(features_X_expanded.columns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# since we have expanded a string of list, we also have to remov \"[\" and \"]\" from first and last column\n",
    "fixed_first_column = features_X_expanded[0].str.replace(\"[\",\"\")\n",
    "fixed_last_column = features_X_expanded[99].str.replace(\"]\",\"\")\n",
    "\n",
    "features_X_expanded[0] = fixed_first_column\n",
    "features_X_expanded[99] = fixed_last_column\n",
    "\n",
    "features_X_expanded.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Perform PCA on all data\n",
    "# first create train and test\n",
    "features_X_expanded[\"Labels\"] = labels_Y # appending labels to not lose their assigned labels when performing data split\n",
    "train, test = train_test_split(features_X_expanded, test_size=0.2)\n",
    "\n",
    "# saving labels\n",
    "train_labels = train.pop(\"Labels\")\n",
    "features_X_expanded = train\n",
    "\n",
    "test_labels = test.pop(\"Labels\")\n",
    "\n",
    "# Scaling features such that they all have a mean of 0 and a variance of 1\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(features_X_expanded)\n",
    "\n",
    "pca = PCA() # can be replaced with \"PCA(n_components=2)\" but need to check variance ratio first\n",
    "pca_data = pca.fit_transform(scaled_data)\n",
    "\n",
    "pca.explained_variance_ratio_ # observing how much each PCA is responsible for the variance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# pca_data_allPCs_df = pd.DataFrame(pca_data, columns = [*range(0, pca_data.shape[1])])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# extracting all PCs data to cvs file\n",
    "# pca_data_allPCs_df.to_csv(\"C:/Users/dnaen/APG_data/pca_data_allPCs.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2) # to be able to do Euclidean distance calculation set to 2\n",
    "pca_data_2PCs = pca.fit_transform(scaled_data)\n",
    "pca_data_2PCs_df = pd.DataFrame(pca_data_2PCs, columns = ['PC1','PC2'])\n",
    "pca_data_2PCs_df[\"Labels\"] = train_labels # can just append it since row order is not affected"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# setting up test\n",
    "scaled_test = scaler.transform(test)\n",
    "pca_test_2PCs = pca.transform(scaled_test)\n",
    "\n",
    "pca_test_2PCs_df = pd.DataFrame(pca_test_2PCs, columns = ['PC1','PC2'])\n",
    "pca_test_2PCs_df[\"Labels\"] = test_labels # can just append it since row order is not affected\n",
    "\n",
    "pca_test_2PCs_df_sorted = pca_test_2PCs_df.sort_values(by=[\"Labels\"])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# to extract file to csv\n",
    "# pca_data_2PCs_df.to_csv(\"C:/Users/dnaen/APG_data/pca_data_2PCs.csv\", index=False)\n",
    "\n",
    "# to extract from csv\n",
    "# pca_data_2PCs_df = pd.read_csv(\".../pca_data_2PCs.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "              PC1       PC2  Labels\n297926  -2.671005 -0.582621      -1\n1392337 -1.088543 -1.647219      -1\n1341028 -6.194049 -2.545746      -1\n180382  -2.528904 -2.998015      -1\n607497  -2.604695 -2.612323      -1\n992916  -3.428668 -2.844442      -1\n1154659 -4.379559 -1.489158      -1\n398164  -2.210939 -2.109961      -1\n1102740 -4.489199  1.498144      -1\n580377  -2.288275 -2.246409      -1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PC1</th>\n      <th>PC2</th>\n      <th>Labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>297926</th>\n      <td>-2.671005</td>\n      <td>-0.582621</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>1392337</th>\n      <td>-1.088543</td>\n      <td>-1.647219</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>1341028</th>\n      <td>-6.194049</td>\n      <td>-2.545746</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>180382</th>\n      <td>-2.528904</td>\n      <td>-2.998015</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>607497</th>\n      <td>-2.604695</td>\n      <td>-2.612323</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>992916</th>\n      <td>-3.428668</td>\n      <td>-2.844442</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>1154659</th>\n      <td>-4.379559</td>\n      <td>-1.489158</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>398164</th>\n      <td>-2.210939</td>\n      <td>-2.109961</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>1102740</th>\n      <td>-4.489199</td>\n      <td>1.498144</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>580377</th>\n      <td>-2.288275</td>\n      <td>-2.246409</td>\n      <td>-1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to use at visit prob. calculation later\n",
    "# pca_data_2PCs_df_sorted = pca_data_2PCs_df.sort_values(by=[\"Labels\"])\n",
    "# pca_data_2PCs_df_sorted.head(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment size:\n",
      "2\n",
      "Accurate prediction with prob.\n",
      "6.951811433505228e-07\n",
      "False prediction with prob.\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# experiment 1 - done for 2 PCs\n",
    "accurate_estimation_prob = 0\n",
    "false_estimation_prob = 0\n",
    "\n",
    "total_accurate_prob = 0\n",
    "total_false_prob = 0\n",
    "\n",
    "# row size (number of data points)\n",
    "row_size = len(pca_test_2PCs_df.axes[0])\n",
    "experiment_size = 2\n",
    "random_list = [] # for experiment\n",
    "for y in range(0, experiment_size):\n",
    "    random_list.append(random.randint(0, row_size))\n",
    "\n",
    "# only using \"pca_data_2comp_df_sorted\" such that labels match\n",
    "for x in random_list:\n",
    "    label = pca_test_2PCs_df_sorted.at[pca_test_2PCs_df_sorted.index[x], \"Labels\"]\n",
    "    if label >= 0:\n",
    "        current_visit = pca_test_2PCs_df_sorted.iloc[x]\n",
    "        current_visit = current_visit.tolist()\n",
    "        current_visit = current_visit[:-1]\n",
    "\n",
    "        estimated_prob, estimated_label = calculate_prob_of_visit_without_sorting(current_visit, pca_test_2PCs_df_sorted)\n",
    "\n",
    "        if estimated_label == label:\n",
    "            total_accurate_prob += estimated_prob\n",
    "        else:\n",
    "            total_false_prob += estimated_prob\n",
    "\n",
    "accurate_estimation_prob = total_accurate_prob / experiment_size\n",
    "false_estimation_prob = total_false_prob / experiment_size\n",
    "\n",
    "print(\"Experiment size:\")\n",
    "print(experiment_size)\n",
    "print(\"Accurate prediction with prob.\")\n",
    "print(accurate_estimation_prob)\n",
    "print(\"False prediction with prob.\")\n",
    "print(false_estimation_prob)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Can save results here\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# to use it for next experiment\n",
    "avg_pca_test_2PCs_df = get_avg_pca(pca_test_2PCs_df)\n",
    "avg_pca_test_2PCs_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# experiment 2 - done for 2 PCs with averaged PCA\n",
    "accurate_estimation_prob = 0\n",
    "false_estimation_prob = 0\n",
    "\n",
    "total_accurate_prob = 0\n",
    "total_false_prob = 0\n",
    "\n",
    "# row size (number of data points)\n",
    "row_size = len(pca_test_2PCs_df_sorted.axes[0])\n",
    "experiment_size = 100\n",
    "random_list = [] # for experiment\n",
    "\n",
    "# for y in range(0, experiment_size):\n",
    "#    random_list.append(random.randint(0, row_size))\n",
    "\n",
    "# only using \"pca_data_2comp_df_sorted\" such that labels match\n",
    "for x in range(0, row_size):\n",
    "    label = pca_test_2PCs_df_sorted.at[pca_test_2PCs_df_sorted.index[x], \"Labels\"]\n",
    "    if label >= 0:\n",
    "        current_visit = pca_test_2PCs_df_sorted.iloc[x]\n",
    "        current_visit = current_visit.tolist()\n",
    "        current_visit = current_visit[:-1]\n",
    "        estimated_prob, estimated_label = calculate_prob_of_visit_with_sorted_avg_pca(current_visit, avg_pca_test_2PCs_df)\n",
    "\n",
    "        if estimated_label == label:\n",
    "            total_accurate_prob += estimated_prob\n",
    "        else:\n",
    "            total_false_prob += estimated_prob\n",
    "\n",
    "accurate_estimation_prob = total_accurate_prob / experiment_size\n",
    "false_estimation_prob = total_false_prob / experiment_size\n",
    "\n",
    "print(\"Experiment size:\")\n",
    "print(experiment_size)\n",
    "print(\"Accurate prediction with prob.\")\n",
    "print(accurate_estimation_prob)\n",
    "print(\"False prediction with prob.\")\n",
    "print(false_estimation_prob)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- without split -\n",
    "Experiment size:\n",
    "1438474\n",
    "Accurate prediction with prob.\n",
    "0.6983720247985018\n",
    "False prediction with prob.\n",
    "0.2959629440643348\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Keeping it just in case\n",
    "\"\"\"\n",
    "# ----------------------------------------\n",
    "classifier = DecisionTreeClassifier()\n",
    "classifier.fit(pca_data, labels_Y) # train classifier\n",
    "\n",
    "# assuming we got new data\n",
    "newdata = []\n",
    "\n",
    "# just transforming to pca, no re-fit again needed\n",
    "scaled_new_data = scaler.transform(newdata)\n",
    "pca_new_data = pca.transform(scaled_new_data)\n",
    "\n",
    "pred_labels = classifier.predict_proba(pca_new_data)\n",
    "\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Splitting the dataset into the training set and test set such that it can be used for classification\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_X, labels_Y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Scaling features such that they all have a mean of 0 and a variance of 1\n",
    "scaler = StandardScaler()\n",
    "scaled_X_train = scaler.fit_transform(X_train)\n",
    "scaled_X_test = scaler.transform(X_test)\n",
    "\n",
    "pca = PCA() # can be replaced with \"PCA(n_components=2)\" if data is too much\n",
    "pca_X_train = pca.fit_transform(scaled_X_train)\n",
    "pca_X_test = pca.transform(scaled_X_test)\n",
    "\n",
    "pca.explained_variance_ratio_ # observing how much each PCA is responsible for the variance\n",
    "\n",
    "# Training, Making Predictions and Performance Evaluation\n",
    "classifier = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "classifier.fit(pca_X_train, y_train)\n",
    "\n",
    "# Predicting the test set results and making performance evaluation\n",
    "y_pred = classifier.predict(pca_X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Accuracy:\")\n",
    "print(accuracy_score(y_test, y_pred))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
