{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from matplotlib import pyplot as plt\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change paths\n",
    "private_scrape = pd.read_excel(\"C:/Users/laure/OneDrive/Documenten/Project 3.1 APG Files/private_scrape_results.xlsx\")\n",
    "public_scrape = pd.read_excel(\"C:/Users/laure/OneDrive/Documenten/Project 3.1 APG Files/public_scrape_results.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Storing all of the records of 'public_scrape' in English in a separate dataframe. 'private_scrape' does not contain any records in\n",
    "# English so this is not needed there.\n",
    "public_scrape_english = public_scrape.loc[public_scrape['full_url'].str.contains(\"english\", case=False)]\n",
    "\n",
    "# Storing all of the records of 'public_scrape' in Dutch in a separate dataframe. \n",
    "public_scrape_dutch = public_scrape.loc[~public_scrape['full_url'].str.contains(\"english\", case=False)]\n",
    "\n",
    "# Obtaining the paragraphs of these scrape results and converting them to lists, making them easier to iterare over.\n",
    "private_corpus = private_scrape.paragraph.tolist()\n",
    "public_corpus = public_scrape_dutch.paragraph.tolist()\n",
    "public_corpus_english = public_scrape_english.paragraph.tolist()\n",
    "\n",
    "# Adding the 'private_corpus' and 'public_corpus' as they are both in Dutch.\n",
    "corpus_dutch = private_corpus + public_corpus\n",
    "corpus_english = public_corpus_english\n",
    "\n",
    "# Defining which words in Dutch we want to remove before starting computing the tf-idf matrix.\n",
    "dutch_stopwords = stopwords.words('dutch')\n",
    "dutch_months = ['Januari', \"Februari\", \"Maart\", \"April\", \"Mei\", \"Juni\", \"Juli\", \"Augustus\", \"September\", \"Oktober\", \"November\", \"December\", 'januari', \"februari\", \"maart\", \"april\", \"mei\", \"juni\", \"juli\", \"augustus\", \"september\", \"oktober\", \"november\", \"december\"]\n",
    "words_to_remove_dutch = dutch_stopwords + dutch_months\n",
    "\n",
    "# Defining which words in English we want to remove before starting computing the tf-idf matrix.\n",
    "english_stopwords = stopwords.words('english')\n",
    "english_months = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\", \"january\", \"february\", \"march\", \"april\", \"may\", \"june\", \"july\", \"august\", \"september\", \"october\", \"november\", \"december\"]\n",
    "words_to_remove_english = english_stopwords + english_months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function reduces the number of words in the passed on 'corpus' by removing the words in the 'words_to_remove',\n",
    "# replacing words on which emphasis is placed, stemming, and removing some unexpected characters.\n",
    "def clean_documents(corpus, words_to_remove, dutch):\n",
    "    for i in range(len(corpus)):\n",
    "        # In order to work with the corpus, we first need to convert it to a string.\n",
    "        document = str(corpus[i])\n",
    "\n",
    "        # Replacing characters on which emphasis was placed. This helps avoiding the word 'maar' to appear both as\n",
    "        # 'maar' and as 'máár' in the final cleaned corpus.\n",
    "        document = document.replace(\"á\", \"a\").replace(\"é\", \"e\").replace(\"è\", \"e\").replace(\"ó\", \"o\").replace(\"ò\", \"o\").replace(\"í\", \"i\").replace(\"u\")\n",
    "\n",
    "        # Splitting our 'document' into a list of words, meaning we can now look at each word individually and\n",
    "        # initializing a new variable 'clean_document' to which we can store all the words we want to retain.\n",
    "        words = document.split()\n",
    "        cleaned_document = []\n",
    "\n",
    "        # First, we start by removing every word in the list that appears in the 'words_to_remove' list and each word\n",
    "        # that either contains or entirely consists of some unexpected characters (such as 'xD' here which is inserted\n",
    "        # by Python when a tab is read from the original scrape results. This removal is done by simply not adding them\n",
    "        # to the 'cleaned_document' variable.\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            if word[-2:] == \"xD\":\n",
    "                word = word[:-2]\n",
    "            cleaned_document.append(word)\n",
    "        \n",
    "        # Now we can join all these words into a list and remove punctuation, numbers and double spaces.\n",
    "        cleaned_document = \" \".join(cleaned_document)\n",
    "        cleaned_document = cleaned_document.translate(str.maketrans(\"\", \"\", (string.punctuation+\"’\")))\n",
    "        cleaned_document = \"\".join([i for i in cleaned_document if not i.isdigit()])\n",
    "        while \"  \" in cleaned_document:\n",
    "            cleaned_document = cleaned_document.replace(\"  \", \" \")\n",
    "        \n",
    "        print(cleaned_document)\n",
    "        # Splitting our 'cleaned_document' into a list of words, meaning we can now look at each word individually and\n",
    "        # initializing a new variable 'stemmed_document' to which we can store all the words we want to retain.\n",
    "        words = cleaned_document.split()\n",
    "        stemmed_document = []\n",
    "        \n",
    "        # First, we check whether the current word does not appear in the 'words_to_remove' list and then we check whether we\n",
    "        # can stem it.\n",
    "        for word in words:\n",
    "            if word not in words_to_remove:\n",
    "                if dutch:\n",
    "                    if \"ing\" in word and word.replace('ing', '') in words:\n",
    "                        stemmed_document.append(word.replace('ing', ''))\n",
    "                    elif \"s\" in word and word.replace('s', '') in words:\n",
    "                        stemmed_document.append(word.replace('s', ''))\n",
    "                    elif \"ig\" in word and word.replace('ig', '') in words:\n",
    "                        stemmed_document.append(word.replace('ig', ''))\n",
    "                    elif \"isme\" in word and word.replace('isme', '') in words:\n",
    "                        stemmed_document.append(word.replace('isme', ''))\n",
    "                    elif \"lijk\" in word and word.replace('lijk', '') in words:\n",
    "                        stemmed_document.append(word.replace('lijk', ''))\n",
    "                    else:\n",
    "                        stemmed_document.append(word)\n",
    "                elif not dutch:\n",
    "                    if \"s\" in word and word.replace('s', '') in words:\n",
    "                        stemmed_document.append(word.replace('s', ''))\n",
    "                    elif \"ism\" in word and word.replace('ism', '') in words:\n",
    "                        stemmed_document.append(word.replace('ism', ''))\n",
    "                    elif \"ed\" in word and word.replace('ed', '') in words:\n",
    "                        stemmed_document.append(word.replace('ed', ''))\n",
    "                    elif \"al\" in word and word.replace('al', '') in words:\n",
    "                        stemmed_document.append(word.replace('al', ''))\n",
    "                    elif \"ist\" in word and word.replace('ist', '') in words:\n",
    "                        stemmed_document.append(word.replace('ist', ''))\n",
    "                    elif \"ity\" in word and word.replace('ity', '') in words:\n",
    "                        stemmed_document.append(word.replace('ity', ''))\n",
    "                    elif \"ness\" in word and word.replace('ness', '') in words:\n",
    "                        stemmed_document.append(word.replace('ness', ''))\n",
    "                    else:\n",
    "                        stemmed_document.append(word)\n",
    "\n",
    "        stemmed_document = \" \".join(stemmed_document)\n",
    "        corpus[i] = stemmed_document\n",
    "    return corpus\n",
    "\n",
    "# Cleaning the records in Dutch and the ones in English and later combining them to create a single corpus.\n",
    "clean_corpus_dutch = clean_documents(corpus_dutch, words_to_remove_dutch, True)\n",
    "clean_corpus_english = clean_documents(corpus_english, words_to_remove_english, False)\n",
    "clean_corpus = clean_corpus_dutch + clean_corpus_english\n",
    "print(clean_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This max_df value seems not to give the wished results, if max_df=0.3, only words that appear in <30% of all docs should be considered\n",
    "vectorizer = TfidfVectorizer(max_df=1.0)\n",
    "vec_trained = vectorizer.fit_transform(clean_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "cm\n",
    "cmhf\n",
    "cms\n",
    "cnv\n",
    "cnvccoop\n",
    "co\n",
    "'''\n",
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_trained.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_length = []\n",
    "num_above_10k = 0\n",
    "\n",
    "largest_doc_index = None\n",
    "largest_doc_length = -1\n",
    "for i in range(len(clean_corpus)):\n",
    "    doc = clean_corpus[i]\n",
    "    doc_length.append(len(doc.split()))\n",
    "    if len(doc)>largest_doc_length:\n",
    "        largest_doc_index=i\n",
    "        largest_doc_length = len(doc)\n",
    "    if len(doc)>20000:\n",
    "        num_above_10k+=1\n",
    "\n",
    "print(largest_doc_index)\n",
    "plt.hist(doc_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}