{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\01din\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from matplotlib import pyplot as plt\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/laure/OneDrive/Documenten/Project 3.1 APG Files/private_scrape_results.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [2], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m#Change paths\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m private_scrape \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_excel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mC:/Users/laure/OneDrive/Documenten/Project 3.1 APG Files/private_scrape_results.xlsx\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m public_scrape \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_excel(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mC:/Users/laure/OneDrive/Documenten/Project 3.1 APG Files/public_scrape_results.xlsx\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\OneDrive\\Documenten\\PROJECT2\\Project2-2\\3-1-APG-Digital-Exit-Strategy\\venv\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001B[0m, in \u001B[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    209\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    210\u001B[0m         kwargs[new_arg_name] \u001B[38;5;241m=\u001B[39m new_arg_value\n\u001B[1;32m--> 211\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\OneDrive\\Documenten\\PROJECT2\\Project2-2\\3-1-APG-Digital-Exit-Strategy\\venv\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001B[0m, in \u001B[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m>\u001B[39m num_allow_args:\n\u001B[0;32m    326\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    327\u001B[0m         msg\u001B[38;5;241m.\u001B[39mformat(arguments\u001B[38;5;241m=\u001B[39m_format_argument_list(allow_args)),\n\u001B[0;32m    328\u001B[0m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[0;32m    329\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39mfind_stack_level(),\n\u001B[0;32m    330\u001B[0m     )\n\u001B[1;32m--> 331\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\OneDrive\\Documenten\\PROJECT2\\Project2-2\\3-1-APG-Digital-Exit-Strategy\\venv\\lib\\site-packages\\pandas\\io\\excel\\_base.py:482\u001B[0m, in \u001B[0;36mread_excel\u001B[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, decimal, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001B[0m\n\u001B[0;32m    480\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(io, ExcelFile):\n\u001B[0;32m    481\u001B[0m     should_close \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m--> 482\u001B[0m     io \u001B[38;5;241m=\u001B[39m \u001B[43mExcelFile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mio\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mengine\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    483\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m engine \u001B[38;5;129;01mand\u001B[39;00m engine \u001B[38;5;241m!=\u001B[39m io\u001B[38;5;241m.\u001B[39mengine:\n\u001B[0;32m    484\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    485\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEngine should not be specified when passing \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    486\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124man ExcelFile - ExcelFile already has the engine set\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    487\u001B[0m     )\n",
      "File \u001B[1;32m~\\OneDrive\\Documenten\\PROJECT2\\Project2-2\\3-1-APG-Digital-Exit-Strategy\\venv\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1652\u001B[0m, in \u001B[0;36mExcelFile.__init__\u001B[1;34m(self, path_or_buffer, engine, storage_options)\u001B[0m\n\u001B[0;32m   1650\u001B[0m     ext \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxls\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1651\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1652\u001B[0m     ext \u001B[38;5;241m=\u001B[39m \u001B[43minspect_excel_format\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1653\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcontent_or_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\n\u001B[0;32m   1654\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1655\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ext \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1656\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1657\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExcel file format cannot be determined, you must specify \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1658\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124man engine manually.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1659\u001B[0m         )\n",
      "File \u001B[1;32m~\\OneDrive\\Documenten\\PROJECT2\\Project2-2\\3-1-APG-Digital-Exit-Strategy\\venv\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1525\u001B[0m, in \u001B[0;36minspect_excel_format\u001B[1;34m(content_or_path, storage_options)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(content_or_path, \u001B[38;5;28mbytes\u001B[39m):\n\u001B[0;32m   1523\u001B[0m     content_or_path \u001B[38;5;241m=\u001B[39m BytesIO(content_or_path)\n\u001B[1;32m-> 1525\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1526\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcontent_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\n\u001B[0;32m   1527\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m handle:\n\u001B[0;32m   1528\u001B[0m     stream \u001B[38;5;241m=\u001B[39m handle\u001B[38;5;241m.\u001B[39mhandle\n\u001B[0;32m   1529\u001B[0m     stream\u001B[38;5;241m.\u001B[39mseek(\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[1;32m~\\OneDrive\\Documenten\\PROJECT2\\Project2-2\\3-1-APG-Digital-Exit-Strategy\\venv\\lib\\site-packages\\pandas\\io\\common.py:865\u001B[0m, in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    856\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(\n\u001B[0;32m    857\u001B[0m             handle,\n\u001B[0;32m    858\u001B[0m             ioargs\u001B[38;5;241m.\u001B[39mmode,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    861\u001B[0m             newline\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    862\u001B[0m         )\n\u001B[0;32m    863\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    864\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[1;32m--> 865\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    866\u001B[0m     handles\u001B[38;5;241m.\u001B[39mappend(handle)\n\u001B[0;32m    868\u001B[0m \u001B[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001B[39;00m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'C:/Users/laure/OneDrive/Documenten/Project 3.1 APG Files/private_scrape_results.xlsx'"
     ]
    }
   ],
   "source": [
    "#Change paths\n",
    "private_scrape = pd.read_excel(\"C:/Users/laure/OneDrive/Documenten/Project 3.1 APG Files/private_scrape_results.xlsx\")\n",
    "public_scrape = pd.read_excel(\"C:/Users/laure/OneDrive/Documenten/Project 3.1 APG Files/public_scrape_results.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Storing all of the records of 'public_scrape' in English in a separate dataframe. 'private_scrape' does not contain any records in\n",
    "# English so this is not needed there.\n",
    "public_scrape_english = public_scrape.loc[public_scrape['full_url'].str.contains(\"english\", case=False)]\n",
    "\n",
    "# Storing all of the records of 'public_scrape' in Dutch in a separate dataframe. \n",
    "public_scrape_dutch = public_scrape.loc[~public_scrape['full_url'].str.contains(\"english\", case=False)]\n",
    "\n",
    "# Obtaining the paragraphs of these scrape results and converting them to lists, making them easier to iterare over.\n",
    "private_corpus = private_scrape.paragraph.tolist()\n",
    "public_corpus = public_scrape_dutch.paragraph.tolist()\n",
    "public_corpus_english = public_scrape_english.paragraph.tolist()\n",
    "\n",
    "# Adding the 'private_corpus' and 'public_corpus' as they are both in Dutch.\n",
    "corpus_dutch = private_corpus + public_corpus\n",
    "corpus_english = public_corpus_english\n",
    "\n",
    "# Defining which words in Dutch we want to remove before starting computing the tf-idf matrix.\n",
    "dutch_stopwords = stopwords.words('dutch')\n",
    "dutch_months = ['Januari', \"Februari\", \"Maart\", \"April\", \"Mei\", \"Juni\", \"Juli\", \"Augustus\", \"September\", \"Oktober\", \"November\", \"December\", 'januari', \"februari\", \"maart\", \"april\", \"mei\", \"juni\", \"juli\", \"augustus\", \"september\", \"oktober\", \"november\", \"december\"]\n",
    "words_to_remove_dutch = dutch_stopwords + dutch_months\n",
    "\n",
    "# Defining which words in English we want to remove before starting computing the tf-idf matrix.\n",
    "english_stopwords = stopwords.words('english')\n",
    "english_months = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\", \"january\", \"february\", \"march\", \"april\", \"may\", \"june\", \"july\", \"august\", \"september\", \"october\", \"november\", \"december\"]\n",
    "words_to_remove_english = english_stopwords + english_months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function reduces the number of words in the passed on 'corpus' by removing the words in the 'words_to_remove',\n",
    "# replacing words on which emphasis is placed, stemming, and removing some unexpected characters.\n",
    "def clean_documents(corpus, words_to_remove, dutch):\n",
    "    for i in range(len(corpus)):\n",
    "        # In order to work with the corpus, we first need to convert it to a string.\n",
    "        document = str(corpus[i])\n",
    "\n",
    "        # Replacing characters on which emphasis was placed. This helps avoiding the word 'maar' to appear both as\n",
    "        # 'maar' and as 'máár' in the final cleaned corpus.\n",
    "        document = document.replace(\"á\", \"a\").replace(\"é\", \"e\").replace(\"è\", \"e\").replace(\"ó\", \"o\").replace(\"ò\", \"o\").replace(\"í\", \"i\").replace(\"u\")\n",
    "\n",
    "        # Splitting our 'document' into a list of words, meaning we can now look at each word individually and\n",
    "        # initializing a new variable 'clean_document' to which we can store all the words we want to retain.\n",
    "        words = document.split()\n",
    "        cleaned_document = []\n",
    "\n",
    "        # First, we start by removing every word in the list that appears in the 'words_to_remove' list and each word\n",
    "        # that either contains or entirely consists of some unexpected characters (such as 'xD' here which is inserted\n",
    "        # by Python when a tab is read from the original scrape results. This removal is done by simply not adding them\n",
    "        # to the 'cleaned_document' variable.\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            if word[-2:] == \"xD\":\n",
    "                word = word[:-2]\n",
    "            cleaned_document.append(word)\n",
    "        \n",
    "        # Now we can join all these words into a list and remove punctuation, numbers and double spaces.\n",
    "        cleaned_document = \" \".join(cleaned_document)\n",
    "        cleaned_document = cleaned_document.translate(str.maketrans(\"\", \"\", (string.punctuation+\"’\")))\n",
    "        cleaned_document = \"\".join([i for i in cleaned_document if not i.isdigit()])\n",
    "        while \"  \" in cleaned_document:\n",
    "            cleaned_document = cleaned_document.replace(\"  \", \" \")\n",
    "        \n",
    "        print(cleaned_document)\n",
    "        # Splitting our 'cleaned_document' into a list of words, meaning we can now look at each word individually and\n",
    "        # initializing a new variable 'stemmed_document' to which we can store all the words we want to retain.\n",
    "        words = cleaned_document.split()\n",
    "        stemmed_document = []\n",
    "        \n",
    "        # First, we check whether the current word does not appear in the 'words_to_remove' list and then we check whether we\n",
    "        # can stem it.\n",
    "        for word in words:\n",
    "            if word not in words_to_remove:\n",
    "                if dutch:\n",
    "                    if \"ing\" in word and word.replace('ing', '') in words:\n",
    "                        stemmed_document.append(word.replace('ing', ''))\n",
    "                    elif \"s\" in word and word.replace('s', '') in words:\n",
    "                        stemmed_document.append(word.replace('s', ''))\n",
    "                    elif \"ig\" in word and word.replace('ig', '') in words:\n",
    "                        stemmed_document.append(word.replace('ig', ''))\n",
    "                    elif \"isme\" in word and word.replace('isme', '') in words:\n",
    "                        stemmed_document.append(word.replace('isme', ''))\n",
    "                    elif \"lijk\" in word and word.replace('lijk', '') in words:\n",
    "                        stemmed_document.append(word.replace('lijk', ''))\n",
    "                    else:\n",
    "                        stemmed_document.append(word)\n",
    "                elif not dutch:\n",
    "                    if \"s\" in word and word.replace('s', '') in words:\n",
    "                        stemmed_document.append(word.replace('s', ''))\n",
    "                    elif \"ism\" in word and word.replace('ism', '') in words:\n",
    "                        stemmed_document.append(word.replace('ism', ''))\n",
    "                    elif \"ed\" in word and word.replace('ed', '') in words:\n",
    "                        stemmed_document.append(word.replace('ed', ''))\n",
    "                    elif \"al\" in word and word.replace('al', '') in words:\n",
    "                        stemmed_document.append(word.replace('al', ''))\n",
    "                    elif \"ist\" in word and word.replace('ist', '') in words:\n",
    "                        stemmed_document.append(word.replace('ist', ''))\n",
    "                    elif \"ity\" in word and word.replace('ity', '') in words:\n",
    "                        stemmed_document.append(word.replace('ity', ''))\n",
    "                    elif \"ness\" in word and word.replace('ness', '') in words:\n",
    "                        stemmed_document.append(word.replace('ness', ''))\n",
    "                    else:\n",
    "                        stemmed_document.append(word)\n",
    "\n",
    "        stemmed_document = \" \".join(stemmed_document)\n",
    "        corpus[i] = stemmed_document\n",
    "    return corpus\n",
    "\n",
    "# Cleaning the records in Dutch and the ones in English and later combining them to create a single corpus.\n",
    "clean_corpus_dutch = clean_documents(corpus_dutch, words_to_remove_dutch, True)\n",
    "clean_corpus_english = clean_documents(corpus_english, words_to_remove_english, False)\n",
    "clean_corpus = clean_corpus_dutch + clean_corpus_english\n",
    "print(clean_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This max_df value seems not to give the wished results, if max_df=0.3, only words that appear in <30% of all docs should be considered\n",
    "vectorizer = TfidfVectorizer(max_df=1.0)\n",
    "vec_trained = vectorizer.fit_transform(clean_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "cm\n",
    "cmhf\n",
    "cms\n",
    "cnv\n",
    "cnvccoop\n",
    "co\n",
    "'''\n",
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_trained.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_length = []\n",
    "num_above_10k = 0\n",
    "\n",
    "largest_doc_index = None\n",
    "largest_doc_length = -1\n",
    "for i in range(len(clean_corpus)):\n",
    "    doc = clean_corpus[i]\n",
    "    doc_length.append(len(doc.split()))\n",
    "    if len(doc)>largest_doc_length:\n",
    "        largest_doc_index=i\n",
    "        largest_doc_length = len(doc)\n",
    "    if len(doc)>20000:\n",
    "        num_above_10k+=1\n",
    "\n",
    "print(largest_doc_index)\n",
    "plt.hist(doc_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
