{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "TF-IDF class. Contains methods which can easily access relevant information from the csv file which the class reads, which represents the TF-IDF model.\n",
    "\n",
    "TF-IDF stands for Term Frequency - Inverse Document Frequency. It gives each keyword-document combination a weight, based on how often it occurs in the document, but also based on how often it occurs in all documents. This ensures that words which occur often in many documents do not receive high weights, rather only words which are relatively unique to a document and occur frequently in that document get high weights.\n",
    "\n",
    "If all documents are about topic A, we do not care that document x contains is about document A.\n",
    "\n",
    "Implementation wise, the TF-IDF is a csv file with each row representing a document and each column representing a keyword, the value at i,j represents the weight that keyword j has in document i."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfIdf:\n",
    "    def __init__(self):\n",
    "        # Loading in the tf-idf\n",
    "        # Rows are documents, the first column is the document id\n",
    "        # Columns are keywords, the first row is the keyword id\n",
    "        self.tfidf = pd.read_csv(\"../data/tf_idf/tf_idf_new.csv\")\n",
    "\n",
    "        # Loading in the keywords\n",
    "        # Two columns, column 1 is id, column 2 is keyword\n",
    "        # We can access the weight in the tf-idf by first accessing the id number from the keyword file\n",
    "        self.tfidf_keywords = pd.read_csv(\"../data/tf_idf/tf_idf_keywords_new.csv\")\n",
    "        self.tfidf_keywords.columns = [\"id\", \"keyword\"]\n",
    "\n",
    "    def get_id_by_keyword(self, keyword):\n",
    "        return self.tfidf_keywords.id.iloc[self.tfidf_keywords[self.tfidf_keywords.keyword == keyword].index].values[0]\n",
    "\n",
    "\n",
    "    def get_keyword_by_id(self, id):\n",
    "        return self.tfidf_keywords.loc[self.tfidf_keywords.id==id, 'keyword'].iloc[0]\n",
    "    # Method to get the tfidf value of a keyword in a page\n",
    "    def get_tf_idf_value(self, page_id, keyword):\n",
    "        keyword_id = self.get_id_by_keyword(keyword)\n",
    "        return self.tfidf.iloc[page_id-1, keyword_id + 1]\n",
    "\n",
    "    def get_all_keywords_by_id(self, page_id):\n",
    "        keywords = self.tfidf.iloc[page_id-1][1:].values\n",
    "        return keywords\n",
    "\n",
    "    def get_all_keywords_by_id_normalized(self, page_id):\n",
    "        keywords_weights = self.get_all_keywords_by_id(page_id)\n",
    "        sum_value = sum(keywords_weights)\n",
    "        keywords_weights_normalized = keywords_weights/sum_value\n",
    "        return keywords_weights_normalized\n",
    "\n",
    "    def get_number_of_keywords(self):\n",
    "        return self.tfidf_keywords.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Instantiate the object"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfIdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Read in the dataset of visits, used to test the information need prediction."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "generic = lambda x: literal_eval(x)\n",
    "\n",
    "conv = {'url_id_path': generic,\n",
    "        'seconds_spent_path': generic}\n",
    "df = pd.read_csv('../data/clickdata/dataNoUnscrapedVisitsOrUnder20Sec.csv', converters=conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create seperate dataframes for the paths taken and for the seconds spent on each url.\n",
    "In paths, index 0 gives a list of urls, the equivalent index 0 in seconds represents how many seconds the user spent on the equal index urls."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "paths = df.url_id_path\n",
    "seconds = df.seconds_spent_path"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0    [188, 1557, 3, 1, 13, 14, 21, 16, 14, 18, 14, ...\n1                               [1557, 3, 1, 13, 1556]\n2                                      [188, 194, 784]\n3                                    [23, 1557, 3, 13]\n4        [1557, 3, 1, 13, 1, 1559, 12, 1559, 17, 1556]\n5                                      [186, 217, 186]\n6                                                [188]\n7                                [1557, 3, 13, 23, 13]\n8                             [23, 1557, 23, 1557, 13]\n9                                [1557, 1, 1556, 1557]\nName: url_id_path, dtype: object"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0    [4, 17, 5, 7, 31, 27, 126, 55, 9, 13, 3, 328, 5]\n1                                   [14, 4, 4, 33, 0]\n2                                          [5, 20, 0]\n3                                    [0, 163, 4, 151]\n4            [12, 14, 34, 66, 358, 9, 18, 19, 908, 0]\n5                                        [14, 868, 0]\n6                                                [40]\n7                                  [71, 7, 751, 1, 9]\n8                                  [1, 298, 1, 35, 6]\n9                                    [27, 15, 12, 74]\nName: seconds_spent_path, dtype: object"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seconds.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Simplified implementation of IUNIS, which simply looks at the keywords which the user comes across, does not use the adjacency matrix, proximal cues, and spreading activation.\n",
    "It seems that these are mostly important for WUFIS, which we are not interested in, thus implementing it is not worth the effort.\n",
    "\n",
    "Input is the ordered list of page id's that the user has visited.\n",
    "For each page, take the TF-IDF values from that page, which is a row of keywords, with either 0 if the keyword is not present or [0,1] if it is.\n",
    "\n",
    "Then, for each next page, we add the TF-IDF values onto the existing weights.\n",
    "\n",
    "Eventually we can sort the list and we will have the keywords with the highest weight on top, which then represents the most relevant keywords in the visit up until now, and acts as our prediction for what the user is looking for.\n",
    "Instead of only looking at the user alone, we can also compare their information need to information needs of other users, and by looking at where the other users end up, we can predict where our user might want to end up.\n",
    "\n",
    "Optionally, a decay factor can be introdced. Essentially, every iteration, all values are divided by a factor, meaning that keywords on recent pages are more biased and weighted more heavily.\n",
    "\n",
    "It is also possible to give more weight to pages based on how much time the user spent on that page."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "#Method which essentially just takes the keywords and their weights on each page the user visits, continues to the next url, and then sums the weights.\n",
    "#Be sure to realise that only passing a path will execute the normal implementation, which simply sums all new tf-idf values of each url.\n",
    "#This is because the default decay_factor value is 1, meaning that nothing changes, and the default secs is None, if that is None, the if statement with secs is never entered and we do not take time into account.\n",
    "#Sorted gives an option to sort the vector before returning, this puts the most relevant keywords on top, but does not allow for easy similarity comparison, thus should only be used if this is not the intention\n",
    "def find_keyword_weights(path, decay_factor=1, secs=None, sorted=False):\n",
    "    #initiate array of weights (weights start as 0, size=num of keywords)\n",
    "    weights = np.zeros(tfidf.get_number_of_keywords())\n",
    "    #ensures all output is visible\n",
    "    pd.options.display.max_rows = 0\n",
    "    #Iterate over each page url\n",
    "    for i in range(len(path)):\n",
    "        #Divide the weights by the decay factor (1 by default, no decay in that case)\n",
    "        weights = weights/decay_factor\n",
    "        #Initialize the tf-idf weights of the current page\n",
    "        new_weights = tfidf.get_all_keywords_by_id_normalized(path[i])\n",
    "        #If we are taking time into account, multiply the weights by the amount of time spent on this page\n",
    "        if secs is not None:\n",
    "            new_weights*=secs[i]\n",
    "        #Add the new weights to the existing weights\n",
    "        weights += new_weights\n",
    "        #Normalize such that the largest value becomes 1\n",
    "        weights /= max(weights)\n",
    "    #Option to sort s.t. max weights are on top.\n",
    "    if sorted:\n",
    "        df = DataFrame(weights, columns=['weights'])\n",
    "        return df.sort_values(by=['weights'], ascending=False)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a method to print out the top 'num_of_words' weighted keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_weights_as_words(num_of_words, weights):\n",
    "    top_weights = weights.head(num_of_words)\n",
    "    i=0\n",
    "    for index in top_weights.index:\n",
    "        print(str(tfidf.get_keyword_by_id(index)) + \" : \" + str(top_weights.weights.iloc[i]))\n",
    "        i+=1\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Line 2 in this section gets rid of all useless pages (login, logout, search, and error)\n",
    "Then, if the user has visited more than 5 pages, it will compute the keywords up until page 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_keywordless_pages(path, seconds):\n",
    "    indices_to_remove = []\n",
    "    for i in range(len(path)):\n",
    "        if path[i]>1494:\n",
    "            indices_to_remove.append(i)\n",
    "    for i in range(len(indices_to_remove)-1, -1, -1):\n",
    "        index = indices_to_remove[i]\n",
    "        path.pop(index)\n",
    "        seconds.pop(index)\n",
    "    return path, seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These sections computes the weights after each different page to see how it changes throughout the visit\n",
    "We can do interesting experiments to see how information need changes throughout the visit, and to find out at which page index we can best make our prediction final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[188, 1557, 3, 1, 13, 14, 21, 16, 14, 18, 14, 5, 1556]\n",
      "[4, 17, 5, 7, 31, 27, 126, 55, 9, 13, 3, 328, 5]\n",
      "[188, 3, 1, 13, 14, 21, 16, 14, 18, 14, 5]\n",
      "[4, 5, 7, 31, 27, 126, 55, 9, 13, 3, 328]\n"
     ]
    }
   ],
   "source": [
    "print(paths[0])\n",
    "print(seconds[0])\n",
    "path, secs = remove_keywordless_pages(paths[0], seconds[0])\n",
    "print(path)\n",
    "print(secs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "varmx : 1.0\n",
      "bloedverdunners : 0.6638049283638775\n",
      "bloed : 0.6373234904540681\n",
      "inkef : 0.6167828914214601\n",
      "het : 0.5733405582255086\n",
      "de : 0.5088098957736215\n",
      "een : 0.48704626216305996\n",
      "patienten : 0.44682922636343525\n",
      "acute : 0.44253661890925144\n",
      "van : 0.4380373283134113\n",
      "\n",
      "\n",
      "varmx : 1.0\n",
      "het : 0.6669760086973442\n",
      "bloedverdunners : 0.6638049283638776\n",
      "bloed : 0.6373234904540682\n",
      "inkef : 0.6167828914214603\n",
      "van : 0.5448678920060637\n",
      "een : 0.5341824083895257\n",
      "de : 0.5240920267861017\n",
      "in : 0.5021834476890716\n",
      "patienten : 0.4468292263634353\n",
      "\n",
      "\n",
      "varmx : 1.0\n",
      "het : 0.6976528647695376\n",
      "bloedverdunners : 0.6638049283638777\n",
      "bloed : 0.6373234904540682\n",
      "de : 0.6292334273439237\n",
      "inkef : 0.6167828914214604\n",
      "een : 0.6113961541610639\n",
      "van : 0.6048676184832219\n",
      "in : 0.5485116951519945\n",
      "uw : 0.4557901062620939\n",
      "\n",
      "\n",
      "varmx : 1.0\n",
      "het : 0.7745330476625701\n",
      "de : 0.749005243795153\n",
      "van : 0.7176429449599722\n",
      "een : 0.7028725065634044\n",
      "uw : 0.6886581729266865\n",
      "bloedverdunners : 0.6638049283638777\n",
      "bloed : 0.6373234904540682\n",
      "inkef : 0.6167828914214604\n",
      "in : 0.6048048350918963\n",
      "\n",
      "\n",
      "varmx : 1.0\n",
      "uw : 0.7984645822161852\n",
      "het : 0.7951307839051615\n",
      "de : 0.7893460295424664\n",
      "een : 0.7858238163566538\n",
      "van : 0.7176429449599723\n",
      "in : 0.6670183174368332\n",
      "bloedverdunners : 0.6638049283638777\n",
      "pensioen : 0.6391072840700966\n",
      "bloed : 0.6373234904540682\n",
      "\n",
      "\n",
      "de : 1.0\n",
      "varmx : 0.9367356039581491\n",
      "het : 0.8512723289371148\n",
      "uw : 0.8188824195564401\n",
      "een : 0.7361091472195478\n",
      "in : 0.678404295137307\n",
      "van : 0.6722416974733845\n",
      "bloedverdunners : 0.6218097104813329\n",
      "pensioen : 0.5986745477374544\n",
      "bloed : 0.5970036047472073\n",
      "\n",
      "\n",
      "de : 1.0\n",
      "het : 0.8860722951011187\n",
      "varmx : 0.7949937984246443\n",
      "uw : 0.7797710928465019\n",
      "een : 0.6338763210058332\n",
      "van : 0.6238547901935643\n",
      "in : 0.6123568385688566\n",
      "pensioen : 0.5968381216237169\n",
      "bloedverdunners : 0.527720801412998\n",
      "bloed : 0.5066682225013323\n",
      "\n",
      "\n",
      "de : 1.0\n",
      "het : 0.8715125310524254\n",
      "uw : 0.855082790488682\n",
      "pensioen : 0.8395141526708072\n",
      "varmx : 0.7641667127888025\n",
      "een : 0.689031556408855\n",
      "over : 0.6821722666060557\n",
      "meer : 0.6716804072285323\n",
      "in : 0.6484128375580563\n",
      "van : 0.5996638781641416\n",
      "\n",
      "\n",
      "pensioen : 1.0\n",
      "uw : 0.9387112963521302\n",
      "de : 0.8309440933437651\n",
      "het : 0.6907543877976331\n",
      "meer : 0.672430025494664\n",
      "in : 0.6322184019956331\n",
      "een : 0.6249821043480545\n",
      "varmx : 0.6056728860000825\n",
      "over : 0.5406846942555827\n",
      "van : 0.5135888914087327\n",
      "\n",
      "\n",
      "pensioen : 1.0\n",
      "uw : 0.8213730042335858\n",
      "meer : 0.7383441779262491\n",
      "de : 0.6825348535668281\n",
      "over : 0.6301073890229666\n",
      "het : 0.5572489887839466\n",
      "een : 0.55457088806598\n",
      "in : 0.5439942501756434\n",
      "ouderdomspensioen : 0.54360234259518\n",
      "keuzepensioen : 0.5077792266509906\n",
      "\n",
      "\n",
      "pensioen : 1.0\n",
      "de : 0.8591798540303274\n",
      "uw : 0.858183729438619\n",
      "meer : 0.8503330191211305\n",
      "over : 0.7418827011579351\n",
      "een : 0.6955894147825206\n",
      "in : 0.5738689906216997\n",
      "het : 0.54219381245347\n",
      "ouderdomspensioen : 0.5289158572248069\n",
      "of : 0.5230607719050732\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(path)):\n",
    "    subpath = path[:i+1]\n",
    "    weights = find_keyword_weights(subpath, decay_factor=7, sorted=True)\n",
    "    print_top_weights_as_words(10, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(path)):\n",
    "    subpath = path[:i+1]\n",
    "    subsecs = secs[:i+1]\n",
    "    weights = find_keyword_weights(subpath, secs=subsecs, decay_factor=1.25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea for evaluation:\n",
    "\n",
    "Look at the keywords at a specific time (we must make sure the weights are normalised, they are not in the implementation above), and compare it with a certain page near the end of the user visit to see if the user found their need.\n",
    "We can do either the last page, or maybe the page that the user has spent the most time on.\n",
    "The problem becomes that we want to evaluate the prediction of the information need, and there will be inaccuracies as the user data is not necessarily correct.\n",
    "But if we can create a metric and we can compare different implementations with the same metric we still have something nice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def estimate_most_important_page(path, seconds):\n",
    "    path = path[int(len(path)/2):]\n",
    "    seconds = seconds[int(len(seconds)/2):]\n",
    "    index = seconds.index(max(seconds))\n",
    "    return path[index]\n",
    "\n",
    "def find_keywords_of_estimated_most_important_page(path, seconds):\n",
    "    id = estimate_most_important_page(path, seconds)\n",
    "    return tfidf.get_all_keywords_by_id_normalized(id)\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "def similarity_estimated_information_need(information_need, keywords_most_important_page):\n",
    "    def cosine_similarity(list_1, list_2):\n",
    "        cos_sim = dot(list_1, list_2) / (norm(list_1) * norm(list_2))\n",
    "        return cos_sim\n",
    "    similarity = cosine_similarity(information_need, keywords_most_important_page)\n",
    "    if math.isnan(similarity):\n",
    "        return 0\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarities(subpath_size):\n",
    "    count=0\n",
    "    similarity_list = []\n",
    "    for j in range(len(paths)):\n",
    "        path, second = remove_keywordless_pages(paths[j], seconds[j])\n",
    "        if count == 5000:\n",
    "            break\n",
    "        if len(path)>5:\n",
    "            count+=1\n",
    "            subpath = path[:subpath_size]\n",
    "            weights = find_keyword_weights(subpath)\n",
    "            similarity_list.append(similarity_estimated_information_need(weights,find_keywords_of_estimated_most_important_page(path, second)))\n",
    "    return similarity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "similarity_scores_basic = []\n",
    "for i in range(1,8):\n",
    "    similarity_scores_basic.append(np.mean(compute_similarities(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(list(range(1,8)), similarity_scores_basic)\n",
    "plt.xlabel(\"Prediction page\")\n",
    "plt.ylabel(\"Similarity\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def compute_similarities_decay(decay_factor):\n",
    "    count=0\n",
    "    similarity_list = []\n",
    "    for j in range(len(paths)):\n",
    "        path, second = remove_keywordless_pages(paths[j], seconds[j])\n",
    "        if count == 5000:\n",
    "            break\n",
    "        if len(path)>5:\n",
    "            count+=1\n",
    "            subpath = path[:3]\n",
    "            weights = find_keyword_weights(subpath, decay_factor=decay_factor)\n",
    "            similarity_list.append(similarity_estimated_information_need(weights, find_keywords_of_estimated_most_important_page(path, second)))\n",
    "    return similarity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "similarity_scores_decay = []\n",
    "\n",
    "for i in range(1,50):\n",
    "    similarity_scores_decay.append(np.mean(compute_similarities_decay(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(list(range(1,50)), similarity_scores_decay)\n",
    "plt.xlabel(\"Factor of Decay\")\n",
    "plt.ylabel(\"Similarity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def compute_similarities_time(subpath_size, decay=1):\n",
    "    count=0\n",
    "    similarity_list = []\n",
    "    for j in range(len(paths)):\n",
    "        path, second = remove_keywordless_pages(paths[j], seconds[j])\n",
    "        if count == 5000:\n",
    "            break\n",
    "        if len(path)>5:\n",
    "            count+=1\n",
    "            subpath = path[:subpath_size]\n",
    "            subsecs = second[:subpath_size]\n",
    "            weights = find_keyword_weights(subpath, secs=subsecs, decay_factor=decay)\n",
    "            similarity_list.append(similarity_estimated_information_need(weights, find_keywords_of_estimated_most_important_page(path, second)))\n",
    "    return similarity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "similarity_scores_time = []\n",
    "for i in range(1,8):\n",
    "    similarity_scores_time.append(np.mean(compute_similarities_time(i)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(similarity_scores_time)\n",
    "plt.plot(list(range(1,8)), similarity_scores_time, label=\"time included\")\n",
    "plt.plot(list(range(1,8)), similarity_scores_basic, label='no time included')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Prediction page\")\n",
    "plt.ylabel(\"Similarity\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "similarity_scores_time_decay = []\n",
    "for i in range(1,50):\n",
    "    similarity_scores_time_decay.append(np.mean(compute_similarities_time(3, i)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(list(range(1,50)), similarity_scores_time_decay, label=\"time included\")\n",
    "plt.plot(list(range(1,50)), similarity_scores_decay, label='no time included')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Factor of Decay\")\n",
    "plt.ylabel(\"Similarity\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "similarity_scores_time_decay_below_one = []\n",
    "for i in values:\n",
    "    similarity_scores_time_decay_below_one.append(np.mean(compute_similarities_time(3, i)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(values, similarity_scores_time_decay_below_one, label=\"time included\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Factor of Decay\")\n",
    "plt.ylabel(\"Similarity\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
